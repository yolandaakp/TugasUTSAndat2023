# -*- coding: utf-8 -*-
"""Practice2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qPZtmY01KXhu62zvkfcNBuLZgyxgFJ8c

## 1. Data Preprocessing

## Overview

Practice for Data Analytic Class in Merdeka Belajar 

## Objecive

• Retrieving data 

• Cleansing, integrating
dan transforming data

• Data Preparation

• Exploratory Data
Analysis

• Data Visualization

Dataset yang diambil merupakan dataset tentang review dari skincare dan produk yang dijual di sephora. Berikut merupakan link dataset yang saya ambil: https://www.kaggle.com/datasets/nadyinky/sephora-products-and-skincare-reviews

**1.1 Importing Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

"""Langkah pertama yaitu import library yang akan digunakan, diantaranya yaitu import pandas, numpy, matplotlib.pyplot dan sklearn.preprocessing

**1.2 Importing Dataset**
"""

!gdown --id 10I92BUg1K0oDMrTjox-Y2H95a0bZuh-7

"""import dataset yang di upload di google drive """

import pandas as pd
dataset = pd.read_csv('reviews_0_250.csv')
dataset

"""Setelah dataset ditampilkan, dapat diketahui bahwa dataset tersebut memiliki 602130 baris dan 19 kolom

## 2. Exploratory Data
"""

dataset.shape

"""syntax ini digunakan untuk menampilkan jumlah baris dan kolom dari dataset yang diambil."""

dataset.columns

"""syntax diatas digunakan untuk menampilkan nama-nama dari kolom dataset yang diambil. Kolom dataset terdiri dari Unnamed: 0, author id, rating, is recommended yang menjelaskan apakah produk ini recommended atau tidak, helpfulness, total feedback count, total neg feedback count, total pos feedback count, submission time, review text, review title, skin tone, eye color, skin type, hair color, product id, product name, brand name, dan price usd."""

dataset.dtypes

"""syntax diatas digunakan untuk menampilkan tipe dari data masing-masing kolom. Terdapat 3 jenis data, yaitu float64 (numerical decimal), int64, dan object (string)."""

dataset.info()

"""dari kolom non-null di atas dapat diketahui data yang angkanya berbeda-beda. hal tersebut menandakan adanya missing value dari data tersebut."""

dataset.nunique()

"""nunique berfungsi untuk menjelaskan jumlah data yang bersifat unik. dari data diatas masih terdapat missing value, jadi perlu untuk melanjutkan ke langkah selanjutnya.

**2.1 Handling Missing Value**

Let's assume unknown value as missing value since unknown means not known or no values
"""

dataset.isnull().sum()

"""syntax digunakan untuk menampilkan kolom-kolom yang terdapat missing value. Dari data yang ditampilkan, terdapat sebanyak 8 baris yang missing value, yaitu is_recommended , helpfulnessm review text, review title, skintone, eyecolor, skintype, dan hair color."""

dataset['is_recommended'].dtypes

dataset['helpfulness'].dtypes

dataset['review_text'].dtypes

dataset['review_title'].dtypes

dataset['skin_tone'].dtypes

dataset['eye_color'].dtypes

dataset['skin_type'].dtypes

dataset['hair_color'].dtypes

"""Untuk data yang memiliki missing value, setelah datanya di cek, kolom dengan type ('O') atau tipe data string adalah kolom city dan prev sold date.

Perform imputer to handle missing value in numerical data
"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values= np.nan, strategy='mean')
imputer

imputer = imputer.fit(dataset[['is_recommended', 'helpfulness']])
imputer

dataset[['is_recommended', 'helpfulness']] = imputer.transform(dataset[['is_recommended', 'helpfulness']])
dataset.isnull().sum()

"""setelah melakukan transform imputer, maka data-data numerical tadi sudah tidak ada missing value, kecuali kolom "review text", "review title", "skin tone", "eye color", "skin type" dan "hair color"

Using drop column tohandle missing value on categorical or object data
"""

dataset = dataset.dropna(axis=1)
dataset.isnull().sum()

"""drop column digunakan untuk menghilangkan missing values yang ada pada data yang memiliki kategori categorical atau object.

drop datetime column since we don't need it, we will use it in the next lecure for forecasting time series data

## 3. Data Visualization

**Eploratory Data Analysis (EDA)**

we will not use exploratory data analysis like in the previous class, we'll take another apporach to visualize data

**3.1 Check Outliers**

Outliers are values at the extreme ends of a dataset.

Some outliers represent true values from natural variation in the population. Other outliers may result from incorrect data entry, equipment malfunctions, or other measurement errors.

An outlier isn’t always a form of dirty or incorrect data, so you have to be careful with them in data cleansing. What you should do with an outlier depends on its most likely cause.

for more info: https://www.scribbr.com/statistics/outliers/
"""

dataset.describe()

"""pada bagian ini mendeskripsikan dataset berdasarkan mean, standar deviasi, min, 25% data, 50% data, 75% data dah nilai max dari dataset tersebut."""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
dataset.plot()

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline

sns.set(rc={'figure.figsize':(11,8)}, font_scale=1.5, style='whitegrid')

# tips = sns.load_dataset(dataset)
sns.boxplot(data=dataset, orient="h");

# Scatter plot
fig, ax = plt.subplots(figsize = (15,7))
ax.scatter(dataset['total_feedback_count'], dataset['total_pos_feedback_count'])
 
# x-axis label
ax.set_xlabel('total_feedback_count')
 
# y-axis label
ax.set_ylabel('total_pos_feedback_count')
plt.show()

"""**Removing the outliers using IQR**

**IQR (Inter Quartile Range)**

IQR = Quartile3 – Quartile1

Quartile description: https://rumusbilangan.com/rumus-kuartil/

IQR is interpolation method to remove outliers

To define the outlier base value is defined above and below datasets normal range namely Upper and Lower bounds, define the upper and the lower bound (1.5*IQR value is considered) :

upper = Q3 +1.5*IQR

lower = Q1 – 1.5*IQR

how to calculate IQR:

https://www.timesmojo.com/what-is-the-15-iqr-rule/

https://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/

In the above formula as according to statistics, the 0.5 scale-up of IQR (new_IQR = IQR + 0.5*IQR) is taken, to consider all the data between 2.7 standard deviations in the Gaussian Distribution.
"""

import sklearn

# IQR Price
Q1 = np.percentile(dataset['total_feedback_count'], 25,
                   method = 'midpoint')
 
Q3 = np.percentile(dataset['total_feedback_count'], 75,
                   method = 'midpoint')
IQR = Q3 - Q1
 
print("Old Shape: ", dataset.shape)

# Upper bound
upper = Q3 + 1.5 * IQR

# Lower bound
lower = Q1 - 1.5 * IQR

# Finding the indices of the outliers
outliers_upper = dataset[dataset['total_feedback_count'] > upper].index
outliers_lower = dataset[dataset['total_feedback_count'] < lower].index

# Concatenate the indices of the outliers
outliers = outliers_upper.append(outliers_lower)

# Removing the outliers
dataset.drop(outliers, inplace=True)

print("New Shape: ", dataset.shape)

import sklearn

# IQR Landsize
Q1 = np.percentile(dataset['total_pos_feedback_count'], 25,
                   method = 'midpoint')
 
Q3 = np.percentile(dataset['total_pos_feedback_count'], 75,
                   method = 'midpoint')
IQR = Q3 - Q1
 
print("Old Shape: ", dataset.shape)

# Upper bound
upper = Q3 + 1.5 * IQR

# Lower bound
lower = Q1 - 1.5 * IQR

# Finding the indices of the outliers
outliers_upper = dataset[dataset['total_pos_feedback_count'] > upper].index
outliers_lower = dataset[dataset['total_pos_feedback_count'] < lower].index

# Concatenate the indices of the outliers
outliers = outliers_upper.append(outliers_lower)

# Removing the outliers
dataset.drop(outliers, inplace=True)

print("New Shape: ", dataset.shape)

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline

sns.set(rc={'figure.figsize':(11,8)}, font_scale=1.5, style='whitegrid')

# tips = sns.load_dataset(dataset)
sns.boxplot(data=dataset, orient="h");

"""data sudah menyeba dengan rata, maka tidak ada outlier"""

# Scatter plot
fig, ax = plt.subplots(figsize = (15,7))
ax.scatter(dataset['total_feedback_count'], dataset['total_pos_feedback_count'])
 
# x-axis label
ax.set_xlabel('total_feedback_count')
 
# y-axis label
ax.set_ylabel('total_pos_feedback_count')
plt.show()

"""Show data distribution based on density (kde - kernel density estimation)"""

dataset.plot(kind='kde',subplots=True,layout=(10,2),figsize=(15,12), sharex=False)
plt.show()

"""Show data distribution based on histogram"""

dataset.plot(kind='hist',subplots=True,layout=(10,2),figsize=(15,12), sharex=False)
plt.show()

dataset.plot(kind='line',subplots=True,layout=(10,2),figsize=(15,12), sharex=False)
plt.show()

"""**3.2 CORRELATION MATRIX**"""

pd.plotting.scatter_matrix(dataset,figsize=(12,12))
plt.show()

"""correlation matrix with heatmap"""

corr_data = dataset
corr = corr_data.corr()

cor_plot = sns.heatmap(corr,annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':10})
fig=plt.gcf()
fig.set_size_inches(15,7)
plt.xticks(fontsize=10,rotation=-30)
plt.yticks(fontsize=10)
plt.title('Correlation Matrix')
plt.show()

"""**4. Splitting Dataset**

**4.1 Slicing**

We define x and y as x for data and y for label
"""

x = dataset.drop('is_recommended', axis=1)
x

"""Choose feature Type as a label"""

y = dataset["is_recommended"]
y

y.values.reshape(-1,1)

"""**4.2 Categorical Encoding**

Choose One Hot Encoding or Label Encoding for perform categorical encoding

**One-Hot Encoding for data input x**
"""

x = pd.get_dummies(x)
x

x.shape

"""**4.2.1 Standarization**

Standardization is used on the data values that are normally distributed. Further, by applying standardization, we tend to make the mean of the dataset as 0 and the standard deviation equivalent to 1.

That is, by standardizing the values, we get the following statistics of the data distribution

mean = 0
standard deviation = 1

Thus, by this the data set becomes self explanatory and easy to analyze as the mean turns down to 0 and it happens to have an unit variance.

References: https://www.askpython.com/python/examples/standardize-data-in-python#:~:text=Ways%20to%20Standardize%20Data%20in%20Python%201%201.,load_iris%20...%202%202.%20Using%20StandardScaler%20%28%29%20function
"""

from sklearn.preprocessing import StandardScaler
std_scale = StandardScaler().fit_transform(x)
std_scale = pd.DataFrame(std_scale)

std_scale

"""**4.2.2 Normalization**

A way to normalize the input features/variables is the Min-Max scaler. By doing so, all features will be transformed into the range [0,1] meaning that the minimum and maximum value of a feature/variable is going to be 0 and 1, respectively.

The main idea behind normalization/standardization is always the same. Variables that are measured at different scales do not contribute equally to the model fitting & model learned function and might end up creating a bias. Thus, to deal with this potential problem feature-wise normalization such as MinMax Scaling is usually used prior to model fitting.

reference : https://stackoverflow.com/questions/62178888/can-someone-explain-to-me-how-minmaxscaler-works

"""

from sklearn.preprocessing import MinMaxScaler
x = MinMaxScaler().fit_transform(x)
x = pd.DataFrame(x)

x

"""**4.3 Splitting**"""

from sklearn.model_selection import train_test_split
xTrain, xTest, yTrain, yTest = train_test_split(x, y, train_size=0.8, random_state=1, stratify=y)

from collections import Counter
print("Train distribution", Counter(yTrain))
print("Test distribution", Counter(yTest))

"""This stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.

For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.

For more: https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn

Tugas praktekkan dengan dataset yg berbeda dari kaggle dan upload di github
"""

from sklearn.linear_model import LogisticRegression

models = []
models.append(('LR', LogisticRegression()))

results_c = []
names_c = []

for name, model in models:
    # define how to split off validation data ('kfold' how many folds)
    kfold = KFold(n_splits=10)    
    # train the model
    cv_results = cross_val_score(model, xTrain, yTrain, cv=kfold, scoring='accuracy')    
    results_c.append(cv_results)
    names_c.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

LR = LogisticRegression()
LR.fit(xTrain, yTrain)

predictions = LR.predict(xTest)
print(accuracy_score(yTest, predictions))

# Confusion Matrix 
from sklearn.metrics import confusion_matrix
import pylab as pl

print(confusion_matrix(yTest, predictions))

cm = confusion_matrix(yTest, predictions)
pl.matshow(cm)
pl.title('Confusion matrix of the classifier')
pl.colorbar()
pl.show()

# Classification Report
from sklearn.metrics import classification_report
print(classification_report(yTest, predictions))

